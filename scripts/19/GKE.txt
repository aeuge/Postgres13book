-- GKE & Postgres
-- simple pod
cat pod.yaml
-- simple replicaset
cat rs.yaml

-- apply config for replicaset
kubectl apply -f rs.yaml
kubectl get all

kubectl delete pod hello-rs-demo-9b7rp
kubectl get all

-- show labels
kubectl get pods --show-labels

-- delete label
kubectl label pod hello-rs-demo-9cdsp app-
kubectl get pods --show-labels

-- look inside pod
kubectl describe pod hello-rs-demo-9cdsp

-- delete replicaset
kubectl delete rs hello-rs-demo

-- delete pod
kubectl delete pod hello-rs-demo-9cdsp


-- deployment
cat deployment.yaml
kubectl apply -f deployment.yaml
kubectl get all


kubectl set image deployment/hello-deployment hello-py=aristoveugene/hello-py:v2 --record
kubectl get all

-- view history
kubectl rollout history deployment/hello-deployment

-- rollback deployment
kubectl rollout undo deployment hello-deployment
kubectl get all

-- increase number of replicas
kubectl scale deployment hello-deployment --replicas=4
kubectl get all

kubectl delete deployment hello-deployment

-- service
cat service.yaml

kubectl apply -f deployment.yaml -f service.yaml
kubectl get all

-- можно прокинуть порт на хостовую машину
kubectl port-forward svc/hello-service 10000:9000
curl http://127.0.0.1:10000/

-- Service discovery
kubectl run -it --rm busybox --image=busybox
env | grep HELLO

wget -qO- http://hello-service.default:9000/

--namespace
kubectl get all -A

-- INGRESS
-- install with helm
cat nginx-ingress.yaml
kubectl create namespace monitoring
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx/
helm repo update
helm install nginx ingress-nginx/ingress-nginx --namespace monitoring -f nginx-ingress.yaml
kubectl get all -n monitoring

-- add ingress
cat ingress.yaml
kubectl apply -f ingress.yaml
kubectl get ing

-- Если просто урлу пойти, то будет 404, потому что Host-а нет.
minikube ip
curl http://192.168.1.73/myapp

curl -H 'Host: hello.world' http://192.168.1.73/myapp

-- можем прописать в хостах
sudo nano /etc/hosts
curl http://hello.world/myapp


-- STATEFULSETS

-- Сначала удалим все. 

kubectl delete all --all

-- Для постгреса сделаем такой stateful сет 
cat postgres.yaml
ccat postgres.yaml

-- Прямо сейчас нет ни persistent volume (хранилища), 
kubectl get pv

-- ни persistent volume claim (запроса на хранилище)
kubectl get pvc

-- применяем манифест. 
kubectl apply -f postgres.yaml

kubectl get all

-- Видим, что создался persistent volume и привязался к pvc 
kubectl get pv
kubectl get pvc

-- Теперь можем зайти в БД по кредам, которые оставляли в ENV переменных контейнера. 
env:
          - name: POSTGRES_DB
            value: myapp
          - name: POSTGRES_USER
            value: myuser
          - name: POSTGRES_PASSWORD
            value: passwd

psql -h 192.168.1.73 -p 31321 -U myuser -W myapp

-- Если удалим под, он все-равно создастся, и данные все равно будут доступны. 
kubectl delete pod postgres-statefulset-0

kubectl get all

psql -h 192.168.1.73 -p 31321 -U myuser -W myapp

-- даже при удалении стейтфуллсет данные остануться на месте
kubectl delete -f postgres.yaml

kubectl get pvc
kubectl get pv



-- GKE
-- создаем вручную кластер, смотрим на параметры


gcloud container clusters list
kubectlG get all

-- посмотрим дефолтный тип стораджа
kubectlG get storageclasses

-- развернем кубик от Севералнайнс

-- по умолчанию ReadWriteMany
-- не работает, поставим ReadWriteOnce и уберем маунт на локальный диск /mnt/data и убираем сторадж моде

nano postgres-configmap.yaml
nano postgres-storage.yaml
nano postgres-deployment.yaml
nano postgres-service.yaml


kubectlG apply -f postgres-configmap.yaml -f postgres-storage.yaml -f postgres-deployment.yaml -f postgres-service.yaml
-- не работает
-- error: unable to recognize "postgres-deployment.yaml": no matches for kind "Deployment" in version "extensions/v1beta1"
apiVersion: extensions/v1beta1  -> apps/v1
nano postgres-deployment.yaml
kubectlG apply -f postgres-deployment.yaml

-- error: error validating "postgres-deployment.yaml": error validating data: ValidationError(Deployment.spec): 
-- missing required field "SELECTor" in io.k8s.api.apps.v1.DeploymentSpec;
nano postgres-deployment.yaml

  SELECTor:
    matchLabels:
      app: postgres

kubectlG apply -f postgres-deployment.yaml

kubectlG get all

kubectlG exec -it pod/postgres-5746888c6d-p9zmt bash
df
/dev/sdb 5гб
psql -U postgres
CREATE DATABASE test;

nano postgres-storage.yaml
-- увеличиваем
kubectlG apply -f postgres-storage.yaml
kubectlG get pvc -o wide
kubectlG get pv -o wide
-- посмотрим, что изменилось
kubectlG exec -it pod/postgres-5746888c6d-p9zmt bash
-- ничего не изменилось %(
mount -f 
-- и вуаля %)
df
psql -U postgres
\l


-- для получения доступа
kubectlG port-forward pod/postgres-5746888c6d-p9zmt bash 5432:5432
-- или
kubectlG port-forward service/postgres 5432:5432

-- пароль 123admin
psql -h localhost -U postgresadmin --password -p 5432 postgresdb

kubectlG get nodes -o wide

-- Для доступа извне нужно юзать LoadBalancer. NodePort только для доступа изнутри GCP
kubectlG get all
-- видим нет внешнего ip
-- посмотрим СТАРЫЙ лоад балансер
nano postgres-service.yaml
-- посмотрим лоад балансер
nano postgres-service2.yaml
kubectlG apply -f postgres-service2.yaml
psql -h 34.68.49.226 -U postgresadmin --password -p 5432 postgresdb


kubectlG delete all --all
-- не забываем про:
kubectlG delete pvc --all
kubectlG delete cm --all
kubectlG delete secrets --all

