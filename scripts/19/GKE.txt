-- GKE & Postgres
-- simple pod
cat pod.yaml
-- simple replicaset
cat rs.yaml

-- apply config for replicaset
kubectl apply -f rs.yaml
kubectl get all

kubectl delete pod hello-rs-demo-9b7rp
kubectl get all

-- show labels
kubectl get pods --show-labels

-- delete label
kubectl label pod hello-rs-demo-9cdsp app-
kubectl get pods --show-labels

-- look inside pod
kubectl describe pod hello-rs-demo-9cdsp

-- delete replicaset
kubectl delete rs hello-rs-demo

-- delete pod
kubectl delete pod hello-rs-demo-9cdsp


-- deployment
cat deployment.yaml
kubectl apply -f deployment.yaml
kubectl get all


kubectl set image deployment/hello-deployment hello-py=aristoveugene/hello-py:v2 --record
kubectl get all

-- view history
kubectl rollout history deployment/hello-deployment

-- rollback deployment
kubectl rollout undo deployment hello-deployment
kubectl get all

-- increase number of replicas
kubectl scale deployment hello-deployment --replicas=4
kubectl get all

kubectl delete deployment hello-deployment

-- service
cat service.yaml

kubectl apply -f deployment.yaml -f service.yaml
kubectl get all

-- можно прокинуть порт на хостовую машину
kubectl port-forward svc/hello-service 10000:9000
curl http://127.0.0.1:10000/

-- Service discovery
kubectl run -it --rm busybox --image=busybox
env | grep HELLO
wget -qO- http://hello-service.myapp:9000/

INGRESS
-- with helm
cat nginx-ingress.yaml
kubectl create namespace monitoring
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx/
helm repo update
helm install nginx ingress-nginx/ingress-nginx --namespace monitoring -f nginx-ingress.yaml
kubectl get pods -n monitoring


cat ingress.yaml
kubectl apply -f ingress.yaml
kubectl get ing

-- Если просто урлу пойти, то будет 404, потому что Host-а нет.
minikube ip
curl http://192.168.1.89/myapp

curl -H 'Host: hello.world' http://192.168.1.89/myapp

-- можем прописать в хостах
sudo nano /etc/hosts
curl http://hello.world/myapp




-- создаем вручную кластер, смотрим на параметры
gcloud container clusters list
kubectlG get all
-- если делать через веб интерфейс ошибка, нужно переинициализировать кластер
-- так как мы делали кластер не через gcloud, доступ мы не получим
-- нужно прописать теперь контекст
-- https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl

gcloud container clusters get-credentials citus --zone us-central1-c
-- посмотрим дефолтный тип стораджа
kubectlG get storageclasses
-- можем сделать свой - например для внешних дисков и т.д.

cd /mnt/c/download/pg25
nano pvc-demo.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-demo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

kubectlG apply -f pvc-demo.yaml
kubectlG get pvc -o wide
kubectlG get pv -o wide
kubectlG describe pv pvc-450a0a68-3bd3-41e3-bbb1-0065960c330f
-- посмотрим диски в GCE
gcloud compute disks list

-- попробуем динамически увеличить размер до 3 Gb
nano pvc-demo.yaml

-- kubectlG CREATE -f pvc-demo.yaml
kubectlG apply -f pvc-demo.yaml

kubectlG get pvc -o wide
kubectlG get pv -o wide
-- заявка на 2, а диск на 3 %)

kubectlG delete -f pvc-demo.yaml


развернем кубик от Севералнайнс
https://severalnines.com/database-blog/using-kubernetes-deploy-postgresql

-- по умолчанию ReadWriteMany
-- не работает, поставим ReadWriteOnce и уберем маунт на локальный диск /mnt/data и убираем сторадж моде

nano postgres-configmap.yaml
nano postgres-storage.yaml
nano postgres-deployment.yaml
nano postgres-service.yaml


kubectlG apply -f postgres-configmap.yaml -f postgres-storage.yaml -f postgres-deployment.yaml -f postgres-service.yaml
-- не работает
-- error: unable to recognize "postgres-deployment.yaml": no matches for kind "Deployment" in version "extensions/v1beta1"
apiVersion: extensions/v1beta1  -> apps/v1
nano postgres-deployment.yaml
kubectlG apply -f postgres-deployment.yaml

-- error: error validating "postgres-deployment.yaml": error validating data: ValidationError(Deployment.spec): 
-- missing required field "SELECTor" in io.k8s.api.apps.v1.DeploymentSpec;
nano postgres-deployment.yaml

  SELECTor:
    matchLabels:
      app: postgres

kubectlG apply -f postgres-deployment.yaml

kubectlG get all

kubectlG exec -it pod/postgres-5746888c6d-p9zmt bash
df
/dev/sdb 5гб
psql -U postgres
CREATE DATABASE test;

nano postgres-storage.yaml
-- увеличиваем
kubectlG apply -f postgres-storage.yaml
kubectlG get pvc -o wide
kubectlG get pv -o wide
-- посмотрим, что изменилось
kubectlG exec -it pod/postgres-5746888c6d-p9zmt bash
-- ничего не изменилось %(
mount -f 
-- и вуаля %)
df
psql -U postgres
\l


-- для получения доступа
kubectlG port-forward pod/postgres-5746888c6d-p9zmt bash 5432:5432
-- или
kubectlG port-forward service/postgres 5432:5432

-- пароль 123admin
psql -h localhost -U postgresadmin --password -p 5432 postgresdb

kubectlG get nodes -o wide

-- Для доступа извне нужно юзать LoadBalancer. NodePort только для доступа изнутри GCP
kubectlG get all
-- видим нет внешнего ip
-- посмотрим СТАРЫЙ лоад балансер
nano postgres-service.yaml
-- посмотрим лоад балансер
nano postgres-service2.yaml
kubectlG apply -f postgres-service2.yaml
psql -h 34.68.49.226 -U postgresadmin --password -p 5432 postgresdb


kubectlG delete all --all
-- не забываем про:
kubectlG delete pvc --all
kubectlG delete cm --all
kubectlG delete secrets --all

